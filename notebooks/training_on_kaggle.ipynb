{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4986623,"sourceType":"datasetVersion","datasetId":2892155},{"sourceId":7690096,"sourceType":"datasetVersion","datasetId":1495143}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/ShowStopperTheSecond/r2d2_for_sar /tmp/r2d2_for_sar","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /tmp/r2d2_for_sar/data/sar/\n!ln -s /kaggle/input/sar-patches/sentinel_1/Training /tmp/r2d2_for_sar/data/sar/optical_flow ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /tmp/r2d2_for_sar","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, pdb\nimport torch\nimport torch.optim as optim\n\nfrom tools import common, trainer\nfrom tools.dataloader import *\nfrom nets.patchnet import *\nfrom nets.losses import *\nimport torchvision.transforms.functional as transform\nfrom PIL import Image, ImageOps\nfrom tqdm.notebook import tqdm\nimport warnings\nfrom  datasets import *\nimport matplotlib.pyplot as plt\nimport cv2\nfrom skimage.measure import ransac\nfrom skimage.feature import match\nfrom skimage.transform import AffineTransform, ProjectiveTransform\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"db_web_images = \"\"\"SyntheticPairDataset(\n    web_images, \n        'RandomScale(256,1024,can_upscale=True)',\n        'RandomTilting(0.5), PixelNoise(.5)')\"\"\"\n\ndb_aachen_images = \"\"\"SyntheticPairDataset(\n    aachen_db_images, \n        'RandomScale(256,1024,can_upscale=True)', \n        'RandomTilting(0.5), PixelNoise(.5)')\"\"\"\n\ndb_aachen_style_transfer = \"\"\"TransformedPairs(\n    aachen_style_transfer_pairs,\n            'RandomScale(256,1024,can_upscale=True), RandomTilting(0.5), PixelNoise(.5)')\"\"\"\n\ndb_aachen_flow = \"aachen_flow_pairs\"\n\n\ndb_sar_images = \"\"\"SyntheticPairDataset(\n    sar_db_images, \n        'RandomScale(256,256,can_upscale=False)', \n        'RandomTilting(0.5), PixelSpeckleNoise(.5)')\"\"\"\n\ndb_sar_flow = \" sar_db_flow\"\n\n\ndefault_dataloader = \"\"\"PairLoader(CatPairDataset(`data`),\n    scale   = 'RandomScale(256,1024,can_upscale=True)',\n    crop    = 'RandomCrop(192)')\"\"\"\n\n\n\n\ndata_sources = dict(\n    W = db_web_images,\n    A = db_aachen_images,\n    F = db_aachen_flow,\n    S = db_aachen_style_transfer,\n    X = db_sar_images,\n    Z = db_sar_flow\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyTrainer(trainer.Trainer):\n    \"\"\" This class implements the network training.\n        Below is the function I need to overload to explain how to do the backprop.\n    \"\"\"\n    def forward_backward(self, inputs):\n        images = [inputs.pop('img1'),inputs.pop('img2')]\n        output = self.net(imgs=images)\n        allvars = dict(inputs, **output)\n        loss, details = self.loss_func(**allvars)\n        if torch.is_grad_enabled(): loss.backward()\n        return loss, details\n\n\ndef load_network(model_fn):\n    checkpoint = torch.load(model_fn, map_location=torch.device('cpu'))\n    print(\"\\n>> Creating net = \" + checkpoint['net'])\n    net = eval(checkpoint['net'])\n    nb_of_weights = common.model_size(net)\n    print(f\" ( Model size: {nb_of_weights/1000:.0f}K parameters )\")\n\n    # initialization\n    weights = checkpoint['state_dict']\n    net.load_state_dict({k.replace('module.',''):v for k,v in weights.items()})\n    return net.eval()\n\n\n# For evaluating\nclass NonMaxSuppression(torch.nn.Module):\n    def __init__(self, rel_thr=0.7, rep_thr=0.7):\n        nn.Module.__init__(self)\n        self.max_filter = torch.nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n        self.rel_thr = rel_thr\n        self.rep_thr = rep_thr\n\n    def forward(self, reliability, repeatability, **kw):\n        assert len(reliability) == len(repeatability) == 1\n        reliability, repeatability = reliability[0], repeatability[0]\n\n        # local maxima\n        maxima = (repeatability == self.max_filter(repeatability))\n\n        # remove low peaks\n        maxima *= (repeatability >= self.rep_thr)\n        maxima *= (reliability >= self.rel_thr)\n\n        return maxima.nonzero().t()[2:4]\n\n\ndef extract_multiscale(net, img, detector, scale_f=2 ** 0.25,\n                       min_scale=0.0, max_scale=1,\n                       min_size=256, max_size=1024,\n                       verbose=False):\n    old_bm = torch.backends.cudnn.benchmark\n    torch.backends.cudnn.benchmark = False  # speedup\n\n    # extract keypoints at multiple scales\n    B, three, H, W = img.shape\n    assert B == 1 and three == 3, \"should be a batch with a single RGB image\"\n\n    assert max_scale <= 1\n    s = 1.0  # current scale factor\n\n    catched = False\n    X, Y, S, C, Q, D = [], [], [], [], [], []\n    while s + 0.001 >= max(min_scale, min_size / max(H, W)):\n        if s - 0.001 <= min(max_scale, max_size / max(H, W)):\n            nh, nw = img.shape[2:]\n            if verbose: print(f\"extracting at scale x{s:.02f} = {nw:4d}x{nh:3d}\")\n            # extract descriptors\n            with torch.no_grad():\n                res = net(imgs=[img])\n\n            # get output and reliability map\n            descriptors = res['descriptors'][0]\n            reliability = res['reliability'][0]\n            repeatability = res['repeatability'][0]\n            if not catched:\n                ret_reliability = reliability.cpu().detach().numpy().squeeze()\n                ret_repeatability = repeatability.cpu().detach().numpy().squeeze()\n                catched = True\n\n            # normalize the reliability for nms\n            # extract maxima and descs\n            y, x = detector(**res)  # nms\n            c = reliability[0, 0, y, x]\n            q = repeatability[0, 0, y, x]\n            d = descriptors[0, :, y, x].t()\n            \n#             d = descriptors[-1][0, :, y, x].t()\n\n            n = d.shape[0]\n\n            # accumulate multiple scales\n            X.append(x.float() * W / nw)\n            Y.append(y.float() * H / nh)\n            S.append((32 / s) * torch.ones(n, dtype=torch.float32, device=d.device))\n            C.append(c)\n            Q.append(q)\n            D.append(d)\n        s /= scale_f\n\n        # down-scale the image for next iteration\n        nh, nw = round(H * s), round(W * s)\n        img = torch.nn.functional.interpolate(img, (nh, nw), mode='bilinear', align_corners=False)\n\n    # restore value\n    torch.backends.cudnn.benchmark = old_bm\n\n    Y = torch.cat(Y)\n    X = torch.cat(X)\n    S = torch.cat(S)  # scale\n    scores = torch.cat(C) * torch.cat(Q)  # scores = reliability * repeatability\n    XYS = torch.stack([X, Y, S], dim=-1)\n    D = torch.cat(D)\n    return XYS, D, scores, ret_reliability, ret_repeatability\n\ndef to_opencv_keypoints(keypoints,size_scale=20):\n    converterd_keypoints=[]\n    for x, y, scale in keypoints:\n        k=cv2.KeyPoint(x=x, y=y, size=scale, angle=0, octave=int(scale))\n        converterd_keypoints.append(k)\n    return converterd_keypoints\ndef translatePoint(p, d):\n    x, y = p\n    dx, dy = d\n    return [x + dx, y + dy]\n\ndef homographyTransform(h,p):\n    res=h@p\n    res=res/res[-1]\n    return res[:2].astype('int')\n\n\n\ndef drawMatches(pt1, img1, pt2, img2, mask, h_mat):\n    h1, w1 = img1.shape[:2]\n    h2, w2 = img2.shape[:2]\n    img = np.zeros(shape=(max(h1, h2), w1 + w2), dtype='uint8')\n    img[:h1, :w1] = img1\n    img[:h2, w1:] = img2\n    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n    x = []\n    x.append([0, 0, 1])\n    x.append([w1, 0, 1])\n    x.append([w1, h1, 1])\n    x.append([0, h1, 1])\n\n    tx = [translatePoint(homographyTransform(h_mat, np.array(p)), (w1, 0)) for p in x]\n    tx = np.array([tx])\n    for i, m in enumerate(mask):\n        if m == 1:\n            dst = (int(w1 + pt2[i][0]), int(pt2[i][1]))\n            src = (int(pt1[i][0]), int(pt1[i][1]))\n            #             color=tuple(np.random.randint(0,255,3,dtype='int'))\n            color = np.random.randint(0, 255, size=(3), dtype=np.uint8)\n            color = (int(color[0]), int(color[1]), int(color[2]))\n            cv2.line(img, src, dst, color, thickness=1)\n    cv2.polylines(img, tx, isClosed=True, color=color, thickness=2)\n    plt.figure(figsize=(20, 18))\n    plt.imshow(img)\n    plt.show()\n    return img\ndef homographyAddTranslation(h, translation):\n    h_t = np.eye(3)\n    h_t[0, 2] = translation[1]\n    h_t[1, 2] = translation[0]\n    return h_t @ h\n\ndef crop_image(img,percentile=.6):\n    height,width=img.shape[:2]\n    h=int(percentile*height/2)\n    w=int(percentile*width/2)\n    return img[h:-h,w:-w]\n\ndef grid_img(img,grid_size=60):\n    data=img.copy()\n    h,w=img.shape[:2]\n    n_h=h//grid_size\n    n_w=w//grid_size\n    skip=-1\n    for m in range(n_h-1):\n        for n in range(n_w-1):\n            skip*=-1\n            if skip==1:\n                data[m*grid_size:(m+1)*grid_size,n*grid_size:(n+1)*grid_size]=0\n    return data\ndef stichImages(img1, img2, h):\n    dst_h, dst_w = img2.shape[:2]\n    dst_shape = (dst_w * 3, dst_h * 3)\n    h = homographyAddTranslation(h, (dst_h, dst_w))\n    img = cv2.warpPerspective(img1, h, dst_shape)\n#     fig, ax = plt.subplots(1, 2, figsize=(20, 18))\n    padded_img = np.pad(img2, [[dst_h, dst_h], [dst_w, dst_w]])\n#     ax[0].imshow(img)\n#     ax[1].imshow(padded_img)\n#     plt.show()\n    img_registeded = padded_img.copy()\n    img=grid_img(img)\n    img_registeded[img != 0] = img[img != 0]\n    img_registeded=crop_image(img_registeded)\n    plt.figure(figsize=(14, 10))\n    plt.imshow(img_registeded)\n    plt.show()\n    return\n\n\n\ndef test_network(img1_rgb, img2_rgb, net, detector, show_net_output=True, show_keypoints=True, show_initial_match=True, show_final_match=True, show_stiche=True):\n    img1 = norm_RGB(img1_rgb)[None].cuda()\n    xys_img1, desc_img1, scores_img1, reliability_img1, repeatability_img1 = extract_multiscale(net, img1, detector,\n                                                                                                min_size=64,\n                                                                                                verbose=False)\n    img2 = norm_RGB(img2_rgb)[None].cuda()\n    xys_img2, desc_img2, scores_img2, reliability_img2, repeatability_img2 = extract_multiscale(net, img2, detector,\n                                                                                                min_size=64,\n                                                                                                verbose=False)\n    if show_net_output:\n        fig, ax = plt.subplots(2, 3, figsize=(20, 8))\n        ax[0, 0].imshow(img1_rgb)\n        pos = ax[0, 1].imshow(reliability_img1)\n        plt.colorbar(pos, ax=ax[0, 1])\n        pos = ax[0, 2].imshow(repeatability_img1)\n        plt.colorbar(pos, ax=ax[0, 2])\n        ax[1, 0].imshow(img2_rgb)\n        pos = ax[1, 1].imshow(reliability_img2)\n        plt.colorbar(pos, ax=ax[1, 1])\n        pos = ax[1, 2].imshow(repeatability_img2)\n        plt.colorbar(pos, ax=ax[1, 2])\n        plt.show()\n\n    kps1 = xys_img1.cpu().detach().numpy()\n    scores1 = scores_img1.cpu().detach().numpy()\n    descs1 = desc_img1.cpu().detach().numpy()\n    kp1 = np.array([kp for kp, score, desc in zip(kps1, scores1, descs1) if score > reliability_thr])\n    desc1 = np.array([desc for kp, score, desc in zip(kps1, scores1, descs1) if score > reliability_thr])\n\n    kps2 = xys_img2.cpu().detach().numpy()\n    scores2 = scores_img2.cpu().detach().numpy()\n    descs2 = desc_img2.cpu().detach().numpy()\n    kp2 = np.array([kp for kp, score, desc in zip(kps2, scores2, descs2) if score > reliability_thr])\n    desc2 = np.array([desc for kp, score, desc in zip(kps2, scores2, descs2) if score > reliability_thr])\n    \n    if show_keypoints:\n        fig, ax = plt.subplots(1 ,2, figsize=(20, 10))\n        if len(kp1) <2 or len(kp2)< 2: return\n        ax[0].imshow(img2_rgb)\n        ax[0].scatter(x=kp2[:, 0], y=kp2[:, 1], marker='x', linewidths=1, c='r', s=150)\n        ax[1].imshow(img1_rgb)\n        ax[1].scatter(x=kp1[:, 0], y=kp1[:, 1], marker='x', linewidths=1, c='r', s=150)\n        plt.show()\n    keys1 = to_opencv_keypoints(kp1)\n    keys2 = to_opencv_keypoints(kp2)\n    matches = match.match_descriptors(desc1, desc2,  max_ratio=.9)\n    good_matches = [cv2.DMatch(m, n, 3) for m, n in matches]\n#     return good_matches, matches\n    img3 = cv2.drawMatches(img1_rgb, keys1, img2_rgb, keys2, good_matches, outImg=None, flags=2)\n    if show_initial_match:\n        plt.figure(figsize=(20, 18))\n        plt.imshow(img3)\n        plt.show()\n        \n    pt1 = [kp1[m.queryIdx][:2] for m in good_matches]\n    pt2 = [kp2[m.trainIdx][:2] for m in good_matches]\n    pt1 = np.array(pt1)\n    pt2 = np.array(pt2)\n#     h, mask = cv2.findHomography(srcPoints=pt1, dstPoints=pt2, method=cv2.FM_RANSAC, ransacReprojThreshold=5)\n    model, inliers = ransac((pt1, pt2), AffineTransform, 3, 5)\n    if show_final_match:\n        imgg = drawMatches(pt1, img1_rgb[...,0], pt2, img2_rgb[..., 0], inliers, model.params)\n        plt.show()\n    if show_stiche:\n        img = cv2.warpPerspective(img1_rgb[...,0], model.params, img2_rgb[...,0].shape[::-1])\n        img_registeded = stichImages(img1_rgb[...,0], img2_rgb[...,0], model.params)\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test images\n# img1 = cv2.imread(\"/kaggle/input/some-registered-images/box.png\")\n# img2 = cv2.imread(\"/kaggle/input/some-registered-images/box_in_scene.png\")\nimg1 = cv2.imread(\"/kaggle/input/some-registered-images/c2.jpg\")\nimg2 = cv2.imread(\"/kaggle/input/some-registered-images/c1.jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reliability_thr = 0.7\nrepeatability_thr = .7\ndetector = NonMaxSuppression(rel_thr = reliability_thr, rep_thr = repeatability_thr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpu = 0\nthreads = 16\nbatch_size = 8\n\n\n# train_data = \"ASF\"\n# train_data = \"F\"\ntrain_data = \"Z\"\n\n\n\ndata_loader = default_dataloader\n\niscuda = common.torch_set_gpu(gpu)\n# iscuda = False\n\n\n# Create data loader\ndb = [data_sources[key] for key in train_data]\nx = data_loader.replace('`data`',','.join(db)).replace('\\n','')\nprint(x)\ndb = eval(data_loader.replace('`data`',','.join(db)).replace('\\n',''))\nprint(\"Training image database =\", db)\nloader = threaded_loader(db, False, threads, batch_size, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(1234)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net = Fast_Quad_L2Net_ConfCFS()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nsampler = \"\"\"NghSampler2(ngh=7, subq=-8, subd=1, pos_d=3, neg_d=5, border=16,\n                            subd_neg=-8,maxpool_pos=False)\"\"\"\nloss =  \"\"\"MultiLoss(\n        1, ReliabilityLoss(`sampler`, base=.5, nq=20),\n        2, CosimLoss(N=`N`),\n        1.5, PeakyLoss(N=`N`),\n        1, SharpenPeak()\n        )\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N = patch_size = 16\nlearning_rate = 1e-4\nweight_decay = 5e-4\nepochs = 800\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create losses\nloss = loss.replace('`sampler`',sampler).replace('`N`',str(patch_size))\nprint(\"\\n>> Creating loss = \" + loss)\nloss = eval(loss.replace('\\n',''))\n\n# create optimizer\noptimizer = optim.Adam( [p for p in net.parameters() if p.requires_grad],\n                        lr=learning_rate, weight_decay=weight_decay)\n\ntrain = MyTrainer(net, loader, loss, optimizer)\nif iscuda: train = train.cuda()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_loss = np.inf\nfor epoch in range(epochs):\n    print(f\"\\n>> Starting epoch {epoch}...\")\n    current_epoch_loss = train()\n    if current_epoch_loss < min_loss:\n        min_loss = current_epoch_loss\n        checkpoint = {\n      'epoch': epoch,\n      'model_state_dict': net.state_dict(), \n      'optimizer_state_dict': optimizer.state_dict()  \n            }\n        torch.save(checkpoint, '/kaggle/working/checkpoint.pt')\n\n    if epoch%20==10:\n        test_network(img2, img1, net, detector)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_network(img1, img2, net, detector)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For resuming the training process later\nnet = Fast_Quad_L2Net_ConfCFS().to('cuda')\noptimizer = optim.Adam( [p for p in net.parameters() if p.requires_grad],\n                        lr=learning_rate, weight_decay=weight_decay)\n\ncheckpoint = torch.load(\"/kaggle/working/checkpoint.pt\")\noptimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\nnet.load_state_dict(checkpoint[\"model_state_dict\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}